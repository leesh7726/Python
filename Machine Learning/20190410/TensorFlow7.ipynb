{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "def train_and_test_data_mnist():\n",
    "\n",
    "    db_path = 'C:\\MLTest\\MNIST'  # TODO: you db path\n",
    "    mnist = input_data.read_data_sets(db_path, one_hot=True)\n",
    "    print('Training data size: ' + str(mnist.train.images.shape))\n",
    "    print('Test data size: ' + str(mnist.test.images.shape))\n",
    "    x_train = mnist.train.images # 55000x784\n",
    "    y_train = mnist.train.labels # 55000x10\n",
    "    x_test = mnist.test.images\n",
    "    y_test = mnist.test.labels\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_placeholders(x_shape, y_shape):\n",
    "    x = tf.placeholder(tf.float32, shape=[None, x_shape]) # 784 for mnist\n",
    "    y = tf.placeholder(tf.float32, shape=[None, y_shape]) # 10 for mnist\n",
    "    return x, y\n",
    "\n",
    "def init_parameters(n_chnls_img, n_chnls_conv1, n_chnls_conv2):\n",
    "\n",
    "    w1 = tf.get_variable('w1', shape=[5, 5, n_chnls_img, n_chnls_conv1],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    w2 = tf.get_variable('w2', shape=[3, 3, n_chnls_conv1, n_chnls_conv2],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    params = {'w1': w1,\n",
    "              'w2': w2}\n",
    "    return params\n",
    "\n",
    "\n",
    "def forward_propagation(x, params):\n",
    "    x = tf.reshape(x, shape=[-1,28,28,1])   # reshape to 28x28. all at once\n",
    "    w1 = params['w1']\n",
    "    w2 = params['w2']\n",
    "\n",
    "    conv1 = tf.nn.conv2d(x, w1, strides=[1,1,1,1], padding='SAME')\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    conv2 = tf.nn.conv2d(pool1, w2, strides=[1,1,1,1], padding='SAME')\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    # flatten the pool2 to feed into the fully connected layer\n",
    "    pool2 = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "    fc1 = tf.contrib.layers.fully_connected(pool2, num_outputs=1024)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob=0.75)\n",
    "    fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=10, activation_fn=None)\n",
    "    return fc2\n",
    "\n",
    "\n",
    "def compute_cost(fc_out, y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc_out, labels=y))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def back_propagation(cost, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def driver_function(x_train, y_train, x_test, y_test, hyper_parameters):\n",
    "    # unpack the network hyper params\n",
    "    learning_rate = hyper_parameters['learning_rate']\n",
    "    n_epochs = hyper_parameters['n_epochs']\n",
    "    batch_size = hyper_parameters['batch_size']\n",
    "    display_step = hyper_parameters['display_step']\n",
    "    n_chnls_img = hyper_parameters['n_chnls_img']\n",
    "    n_chnls_conv1 = hyper_parameters['n_chnls_conv1']\n",
    "    n_chnls_conv2 = hyper_parameters['n_chnls_conv2']\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed=1)\n",
    "    m, x_shape = x_train.shape  # m is the total number of samples\n",
    "    _, y_shape = y_train.shape\n",
    "    x, y = create_placeholders(x_shape, y_shape)\n",
    "    params = init_parameters(n_chnls_img, n_chnls_conv1, n_chnls_conv2)\n",
    "    model = forward_propagation(x, params)\n",
    "    cost = compute_cost(model, y)\n",
    "    optimizer = back_propagation(cost, learning_rate)\n",
    "    all_costs = []              # save errors from all epochs. to plot\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epochs):\n",
    "            num_batch = int(m / batch_size)\n",
    "            count = 0\n",
    "            batch_cost = 0\n",
    "            for nb in range(num_batch):\n",
    "                train_data = x_train[count:count+batch_size]\n",
    "                train_lebels = y_train[count:count+batch_size]\n",
    "                _, tmp_cost = sess.run([optimizer, cost],\n",
    "                                       feed_dict={x:train_data, y:train_lebels})\n",
    "                batch_cost += tmp_cost\n",
    "                count += batch_size\n",
    "            all_costs.append(batch_cost/num_batch)\n",
    "\n",
    "            if epoch % display_step == 0:\n",
    "                print('epoch %d:,  minibatch loss: %f' %(epoch, batch_cost/num_batch))\n",
    "        print('optimization done ...')\n",
    "\n",
    "        # evaluate the model now on the Test Set. images network have not seen so far\n",
    "        correct_pred = tf.equal(tf.arg_max(model, 1), tf.arg_max(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        train_accuracy = accuracy.eval({x: x_train, y:y_train})\n",
    "        test_accuracy = accuracy.eval({x: x_test, y: y_test})\n",
    "\n",
    "    return train_accuracy, test_accuracy, params, all_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 10         # total number of iterations of training\n",
    "batch_size = 100      # or min-batch size\n",
    "display_step = 1      # show prediction errors after each of this many steps\n",
    "\n",
    "n_chnls_img = 1       # mnist images are one channel. 3 for rgb images\n",
    "n_chnls_conv1 = 32    # just a choice\n",
    "n_chnls_conv2 = 64\n",
    "\n",
    "# pack them in a dictionary. sending too many function argument looks clutter\n",
    "hyper_parameters = {'learning_rate': learning_rate, 'n_epochs': n_epochs,\n",
    "                        'batch_size': batch_size, 'display_step': display_step,\n",
    "                        'n_chnls_img': n_chnls_img, 'n_chnls_conv1': n_chnls_conv1,\n",
    "                        'n_chnls_conv2': n_chnls_conv2}\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_and_test_data_mnist()\n",
    "#plt.imshow(x_train[2018, :].reshape((28, 28)), cmap='gray') # look at an image\n",
    "train_acc, test_acc, _, all_costs = driver_function(x_train, y_train,\n",
    "                                                        x_test, y_test,\n",
    "                                                        hyper_parameters)\n",
    "# show the training errors\n",
    "plt.plot(np.squeeze(all_costs))\n",
    "plt.xlabel('iterations');plt.ylabel('cost')\n",
    "plt.title('Learning rate %f' %learning_rate)\n",
    "plt.show()\n",
    "\n",
    "#print('Train accuray: %f' %train_acc)\n",
    "#print('Test accuracy: %f' %test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
